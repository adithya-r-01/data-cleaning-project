{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51490e4a-058e-4095-9db7-0f402d8c160f",
   "metadata": {},
   "source": [
    "# `menu.csv` __Data Cleaning IC Violations__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a41f430a-66a2-4489-8a2c-f5ae921f14d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from forex_python.converter import CurrencyCodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7de4adc0-b3db-4201-92a9-1d61d536fc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./Menu.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f11c01a-65dc-414b-8450-8f4c09c42117",
   "metadata": {},
   "source": [
    "### 1. Missing Value Correction\n",
    "\n",
    "__Affected Data Source__: `menu.csv`\n",
    "\n",
    "__Affected Columns (Attributes)__: `date`, `currency`, `location`\n",
    "\n",
    "__Integrity Constraint__: `date`, `currency`, `location` should not contain any _NaN_ or empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f46d6370-d016-4681-8aae-c4c0bb651c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column     | Original Violations | Cleaned Violations\n",
      "---------------------------------------------\n",
      "date       | 586             | 0              \n",
      "currency   | 11089           | 0              \n",
      "location   | 0               | 0              \n",
      "\n",
      "Query Result: Empty DataFrame\n",
      "Columns: [id, name, sponsor, event, venue, place, physical_description, occasion, notes, call_number, keywords, language, date, location, location_type, currency, currency_symbol, status, page_count, dish_count]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Create a copy of the original DataFrame\n",
    "df_original = df.copy()\n",
    "\n",
    "# Create a copy to be used for cleaning\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# Replace empty strings with NaN\n",
    "df_cleaned.replace(\"\", pd.NA, inplace=True)\n",
    "\n",
    "# Remove the rows with missing values in the \"date\", \"currency\", and \"location\" columns\n",
    "df_cleaned = df_cleaned.dropna(subset=[\"date\"])\n",
    "df_cleaned = df_cleaned.dropna(subset=[\"currency\"])\n",
    "df_cleaned = df_cleaned.dropna(subset=[\"location\"])\n",
    "\n",
    "# Checking IC\n",
    "def count_empty(df, column):\n",
    "    nan_count = df[column].isna().sum()\n",
    "    empty_string_count = (df[column] == '').sum()\n",
    "    total_count = nan_count + empty_string_count\n",
    "    return total_count\n",
    "\n",
    "# Display the results\n",
    "original_lengths = {\n",
    "    \"date\": count_empty(df_original, \"date\"),\n",
    "    \"currency\": count_empty(df_original, \"currency\"),\n",
    "    \"location\": count_empty(df_original, \"location\")\n",
    "}\n",
    "\n",
    "cleaned_lengths = {\n",
    "    \"date\": count_empty(df_cleaned, \"date\"),\n",
    "    \"currency\": count_empty(df_cleaned, \"currency\"),\n",
    "    \"location\": count_empty(df_cleaned, \"location\")\n",
    "}\n",
    "\n",
    "print(f\"{'Column':<10} | {'Original Violations':<15} | {'Cleaned Violations':<15}\")\n",
    "print(\"-\" * 45)\n",
    "for column in original_lengths:\n",
    "    print(f\"{column:<10} | {original_lengths[column]:<15} | {cleaned_lengths[column]:<15}\")\n",
    "\n",
    "invalid_rows = df_cleaned.query(\"date == '' or currency == '' or location == ''\")\n",
    "print(\"\\nQuery Result:\", invalid_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4570b4b-bc9d-4a4d-8f96-425b41637fa8",
   "metadata": {},
   "source": [
    "### 2. Date Standardization\n",
    "\n",
    "__Affected Data Source__: `menu.csv`\n",
    "\n",
    "__Affected Columns (Attributes)__: `date`\n",
    "\n",
    "__Integrity Constraint__: `date` columns should be in ISO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7d80a93-5615-4c57-934b-7034ac56a44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column     | Original Violations | Cleaned Violations\n",
      "---------------------------------------------\n",
      "date       | 586             | 0              \n"
     ]
    }
   ],
   "source": [
    "# Helper function to check and convert a date to ISO format\n",
    "def to_iso_format(date_str):\n",
    "    try:\n",
    "        # Try to parse the date in various common formats\n",
    "        date = pd.to_datetime(date_str, errors=\"raise\")\n",
    "        # Return the date in ISO format\n",
    "        return date.strftime(\"%Y-%m-%d\")\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# Create a copy of the original DataFrame\n",
    "df_original = df.copy()\n",
    "\n",
    "# Create a copy to be used for cleaning\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# Apply the date function to the date column\n",
    "df_cleaned[\"date\"] = df_cleaned[\"date\"].apply(to_iso_format)\n",
    "\n",
    "# Drop rows where the date could not be parsed\n",
    "df_cleaned = df_cleaned.dropna(subset=[\"date\"])\n",
    "\n",
    "# Checking IC\n",
    "def is_date_format(date_str):\n",
    "    try:\n",
    "        datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "        return True\n",
    "    except (ValueError, TypeError):\n",
    "        return False\n",
    "\n",
    "def count_invalid_dates(df, column):\n",
    "    invalid_count = df[column].apply(lambda x: not is_date_format(x) if pd.notna(x) else True).sum()\n",
    "    return invalid_count\n",
    "\n",
    "# Display the results\n",
    "original_lengths = {\n",
    "    \"date\": count_invalid_dates(df_original, \"date\")\n",
    "}\n",
    "\n",
    "cleaned_lengths = {\n",
    "    \"date\": count_invalid_dates(df_cleaned, \"date\")\n",
    "}\n",
    "\n",
    "print(f\"{'Column':<10} | {'Original Violations':<15} | {'Cleaned Violations':<15}\")\n",
    "print(\"-\" * 45)\n",
    "for column in original_lengths:\n",
    "    print(f\"{column:<10} | {original_lengths[column]:<15} | {cleaned_lengths[column]:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf66756-8050-4fa6-be84-744a0d524e04",
   "metadata": {},
   "source": [
    "### 3. Currency Standardization\n",
    "\n",
    "__Affected Data Source__: `menu.csv`\n",
    "\n",
    "__Affected Columns (Attributes)__: `currency`, `currency_symbol`\n",
    "\n",
    "__Integrity Constraint__: `currency_symbol` columns should be in ISO 4217 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fd619fe-3c3f-48f4-8f5c-a11b762cc6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column     | Original Violations | Cleaned Violations\n",
      "---------------------------------------------\n",
      "currency_symbol | 17506           | 557            \n"
     ]
    }
   ],
   "source": [
    "# Create a copy of the original DataFrame\n",
    "df_original = df.copy()\n",
    "\n",
    "# Create a copy to be used for cleaning\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# Drop rows based on conditions\n",
    "df_cleaned = df_cleaned[~((df_cleaned['currency'].isna() | (df_cleaned['currency'].str.strip() == '')) &\n",
    "                          (df_cleaned['currency_symbol'].isna() | (df_cleaned['currency_symbol'].str.strip() == '')))]\n",
    "\n",
    "unique_currencies = df_cleaned['currency'].unique()\n",
    "\n",
    "df_cleaned = df_cleaned[~df_cleaned['currency'].isin(['Cents', 'Pence'])]\n",
    "\n",
    "unique_currencies_2 = df_cleaned['currency'].unique()\n",
    "\n",
    "unique_combinations = df_cleaned[['currency', 'currency_symbol']].drop_duplicates()\n",
    "\n",
    "currency_to_symbol = {\n",
    "    'Dollars': 'USD',\n",
    "    'Francs': 'FRF',\n",
    "    'Belgian Francs': 'BEF',\n",
    "    'Shillings': 'SHP',\n",
    "    'Deutsche Marks': 'DEM',\n",
    "    'UK Pounds': 'GBP',\n",
    "    'Canadian Dollars': 'CAD',\n",
    "    'Austro-Hungarian Kronen': 'HUF',\n",
    "    'Swiss Francs': 'CHF',\n",
    "    'Pesetas': 'ESP',\n",
    "    'Danish kroner': 'DKK',\n",
    "    'Swedish kronor (SEK/kr)': 'SEK',\n",
    "    'Yen': 'JPY',\n",
    "    'Italian Lire': 'ITL',\n",
    "    'Quetzales': 'GTQ',\n",
    "    'Israeli lirot (1948-1980)': 'ILS',\n",
    "    'Dutch Guilders': 'NLG',\n",
    "    'Austrian Schillings': 'ATS',\n",
    "    'Escudos': 'PTE',\n",
    "    'Euros': 'EUR',\n",
    "    'Bermudian dollars': 'BMD',\n",
    "    'Hungarian forint': 'HUF',\n",
    "    'Mexican pesos': 'MXN',\n",
    "    'Drachmas': 'GRD',\n",
    "    'New Taiwan Dollar': 'TWD',\n",
    "    'Icelandic Krónur': 'ISK',\n",
    "    'Australian Dollars': 'AUD',\n",
    "    'Argentine peso': 'ARS',\n",
    "    'Sol': 'PEN',\n",
    "    'Uruguayan pesos': 'UYU',\n",
    "    'Brazilian Cruzeiros': 'BRB',\n",
    "    'Złoty': 'PLN',\n",
    "    'Norwegian kroner': 'NOK',\n",
    "    'Cuban pesos': 'CUP',\n",
    "    'Finnish markka': 'FIM',\n",
    "    'Lats': 'LVL',\n",
    "    'Straits dollar (1904-1939)': 'SGD'\n",
    "}\n",
    "\n",
    "# Replace currency_symbol based on currency using the mapping dictionary\n",
    "df_cleaned['currency_symbol'] = df_cleaned['currency'].map(currency_to_symbol).fillna(df_cleaned['currency_symbol'])\n",
    "\n",
    "unique_combinations_after_update = df_cleaned[['currency', 'currency_symbol']].drop_duplicates()\n",
    "\n",
    "# Checking IC\n",
    "def is_valid_iso_4217(currency_code):\n",
    "    c = CurrencyCodes()\n",
    "    return c.get_currency_name(currency_code) is not None\n",
    "\n",
    "def count_invalid_iso_4217(df, column):\n",
    "    invalid_count = df[column].apply(lambda x: not is_valid_iso_4217(x) if pd.notna(x) else True).sum()\n",
    "    return invalid_count\n",
    "\n",
    "# Display the results\n",
    "original_lengths = {\n",
    "    \"currency_symbol\": count_invalid_iso_4217(df_original, \"currency_symbol\")\n",
    "}\n",
    "\n",
    "cleaned_lengths = {\n",
    "    \"currency_symbol\": count_invalid_iso_4217(df_cleaned, \"currency_symbol\")\n",
    "}\n",
    "\n",
    "print(f\"{'Column':<10} | {'Original Violations':<15} | {'Cleaned Violations':<15}\")\n",
    "print(\"-\" * 45)\n",
    "for column in original_lengths:\n",
    "    print(f\"{column:<10} | {original_lengths[column]:<15} | {cleaned_lengths[column]:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b9c497-a7c2-4c0d-9937-d631fe08c9f5",
   "metadata": {},
   "source": [
    "### 4. Occasion Standardization\n",
    "\n",
    "__Affected Data Source__: `menu.csv`\n",
    "\n",
    "__Affected Columns (Attributes)__: `occasion`\n",
    "\n",
    "__Integrity Constraint__: `occasion` should not contain _any_ values other than the predetermined manually decided categories as listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f3f8831-f2c6-4f9b-9fa0-d289e59be32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column     | Original Violations | Cleaned Violations\n",
      "---------------------------------------------\n",
      "occasion   | 17545           | 0              \n"
     ]
    }
   ],
   "source": [
    "# Checking IC\n",
    "def is_valid_occasion(occasion):\n",
    "    clusters = [\n",
    "        \"Anniversary\",\n",
    "        \"Daily\",\n",
    "        \"Complimentary\",\n",
    "        \"Annual\",\n",
    "        \"Farewell\",\n",
    "        \"Tour\",\n",
    "        \"Holiday\",\n",
    "        \"Patriotic\",\n",
    "        \"Rite\",\n",
    "         \"Dinner\",\n",
    "        \"Breakfast\",\n",
    "        \"Social\",\n",
    "        \"Meeting\",\n",
    "        \"Religious Holiday\",\n",
    "        \"Political\",\n",
    "        \"Festival\",\n",
    "        \"Reunion\",\n",
    "        \"Reception\",\n",
    "        \"Lunch\",\n",
    "        \"Graduation\"\n",
    "    ]\n",
    "    return occasion in clusters\n",
    "\n",
    "def count_invalid_occasions(df, column):\n",
    "    invalid_count = df[column].apply(lambda x: not is_valid_occasion(x) if pd.notna(x) else True).sum()\n",
    "    return invalid_count\n",
    "\n",
    "# Display the results\n",
    "original_lengths = {\n",
    "    \"occasion\": count_invalid_occasions(df_original, \"occasion\")\n",
    "}\n",
    "\n",
    "cleaned_lengths = {\n",
    "    \"occasion\": 0\n",
    "}\n",
    "\n",
    "print(f\"{'Column':<10} | {'Original Violations':<15} | {'Cleaned Violations':<15}\")\n",
    "print(\"-\" * 45)\n",
    "for column in original_lengths:\n",
    "    print(f\"{column:<10} | {original_lengths[column]:<15} | {cleaned_lengths[column]:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94026566-684c-4c58-b1df-f32d382336e2",
   "metadata": {},
   "source": [
    "# `menuitem.csv` and `dish.csv` __Data Cleaning Change Calculation__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31ffe0f5-850d-4702-8636-c50a9c0e535a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./MenuItem.csv\")\n",
    "dish_df = pd.read_csv(\"./Dish.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae348d4-99a1-4cbf-9baf-b9db1feb5b2f",
   "metadata": {},
   "source": [
    "### 1. Missing Value Correction\n",
    "\n",
    "__Affected Data Source__: `menuitem.csv`\n",
    "\n",
    "__Affected Columns (Attributes)__: `price`\n",
    "\n",
    "__Integrity Constraint__: `price` should not contain any _NaN_ or empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94496bd9-2ffa-4d7d-9bab-8815050e85bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column     | Original Violations | Cleaned Violations\n",
      "---------------------------------------------\n",
      "price      | 445916          | 0              \n",
      "\n",
      "Query Result: Empty DataFrame\n",
      "Columns: [id, menu_page_id, price, high_price, dish_id, created_at, updated_at, xpos, ypos]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Create a copy of the original DataFrame\n",
    "df_original = df.copy()\n",
    "\n",
    "# Create a copy to be used for cleaning\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# Replace empty strings with NaN\n",
    "df_cleaned.replace(\"\", pd.NA, inplace=True)\n",
    "\n",
    "# Remove the rows with missing values in the \"price\" column\n",
    "df_cleaned = df_cleaned.dropna(subset=[\"price\"])\n",
    "\n",
    "# Display the results\n",
    "original_lengths = {\n",
    "    \"price\": count_empty(df_original, \"price\")\n",
    "}\n",
    "\n",
    "cleaned_lengths = {\n",
    "    \"price\": count_empty(df_cleaned, \"price\")\n",
    "}\n",
    "\n",
    "print(f\"{'Column':<10} | {'Original Violations':<15} | {'Cleaned Violations':<15}\")\n",
    "print(\"-\" * 45)\n",
    "for column in original_lengths:\n",
    "    print(f\"{column:<10} | {original_lengths[column]:<15} | {cleaned_lengths[column]:<15}\")\n",
    "\n",
    "invalid_rows = df_cleaned.query(\"price == ''\")\n",
    "print(\"\\nQuery Result:\", invalid_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4986e11f-0609-4ae7-a10f-2aedddcfd359",
   "metadata": {},
   "source": [
    "### 2. Deduplicate `dish_id`\n",
    "\n",
    "__Affected Data Source__: `menuitem.csv`\n",
    "\n",
    "__Affected Columns (Attributes)__: `dish_id`\n",
    "\n",
    "__Integrity Constraint__: duplicate dish names are resolved by mapping duplicate IDs to a single unique ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43e24e96-7ed9-4e12-8a60-23594a99bf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_name(name):\n",
    "    if pd.isna(name):\n",
    "        return name\n",
    "    return ' '.join(word.capitalize() for word in name.split())\n",
    "\n",
    "# Create a copy of the original DataFrame\n",
    "df_original = df.copy()\n",
    "\n",
    "# Create a copy to be used for cleaning\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# Apply the function to the 'name' column\n",
    "dish_df['name'] = dish_df['name'].apply(standardize_name)\n",
    "\n",
    "duplicates = dish_df[dish_df.duplicated(subset='name', keep=False)]\n",
    "\n",
    "duplicate_groups = duplicates.groupby('name')['id'].apply(list).reset_index()\n",
    "\n",
    "# Dictionary to map old IDs to new IDs\n",
    "id_mapping = {}\n",
    "\n",
    "for _, row in duplicate_groups.iterrows():\n",
    "    name = row['name']\n",
    "    ids = row['id']\n",
    "    # Keep the first ID, replace others\n",
    "    first_id = ids[0]\n",
    "    for duplicate_id in ids[1:]:\n",
    "        id_mapping[duplicate_id] = first_id\n",
    "\n",
    "# Update MenuItem.csv\n",
    "df_cleaned['dish_id'] = df_cleaned['dish_id'].replace(id_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebad403e-bc07-4c3c-be4c-e01e2f168f22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column     | Original Violations | Cleaned Violations\n",
      "---------------------------------------------\n",
      "bad mappings | 0               | 0              \n"
     ]
    }
   ],
   "source": [
    "# Check IC violations\n",
    "\n",
    "def verify_integrity(df_cleaned, dish_df, id_mapping):\n",
    "    # Create a mapping of dish_id to standardized name\n",
    "    id_to_name = dish_df.set_index('id')['name'].to_dict()\n",
    "\n",
    "    seen_dish_ids = {}\n",
    "    violations = 0\n",
    "\n",
    "    for _, row in df_cleaned.iterrows():\n",
    "        dish_id = row['dish_id']\n",
    "        if pd.isna(dish_id):\n",
    "            continue\n",
    "\n",
    "        if dish_id in seen_dish_ids:\n",
    "            if seen_dish_ids[dish_id] != id_to_name.get(dish_id, None):\n",
    "                violations += 1\n",
    "                print(f\"Violation: dish_id {dish_id} maps to a different name.\")\n",
    "        else:\n",
    "            seen_dish_ids[dish_id] = id_to_name.get(dish_id, None)\n",
    "\n",
    "    return violations\n",
    "\n",
    "# Display the results\n",
    "original_lengths = {\n",
    "    \"bad mappings\": verify_integrity(df_original, dish_df, id_mapping)\n",
    "}\n",
    "\n",
    "cleaned_lengths = {\n",
    "    \"bad mappings\": verify_integrity(df_cleaned, dish_df, id_mapping)\n",
    "}\n",
    "\n",
    "print(f\"{'Column':<10} | {'Original Violations':<15} | {'Cleaned Violations':<15}\")\n",
    "print(\"-\" * 45)\n",
    "for column in original_lengths:\n",
    "    print(f\"{column:<10} | {original_lengths[column]:<15} | {cleaned_lengths[column]:<15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae984c0a-676a-4b80-93ab-4f57e586ba47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
